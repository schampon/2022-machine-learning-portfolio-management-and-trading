{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from ml4pmt.plot import bar, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the FOMC statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.dataset import load_fomc_statements\n",
    "from ml4pmt.text import show_text\n",
    "statements = load_fomc_statements(force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_text(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_days = ['2008-01-22', '2010-05-09', '2020-03-15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract features from text, the simplest way is to count words. In `scikit-learn`, this is done with the function `CountVectorizer`. A slightly more advanced feature is to select words based on a `TFIDF` score, defined as the product of the term frequency (`TF`) and the inverse document frequency (`IDF`). More precisely, the `TFIDF` score trades off: \n",
    "- the terms that are frequent and therefore important in a corpus: \n",
    "- the terms that appear in almost all documents and therefore are not helping to discriminate across documents. \n",
    "\n",
    "In `TfidfVectorizer`, terms can be filtered additionally with: \n",
    "- a `stop word` list\n",
    "- min and max document frequencies or counts \n",
    "- some token pattern (e.g. that eliminates the short tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             min_df=5, max_df=.8, ngram_range=(1, 3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "X = vectorizer.fit_transform(statements['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = vectorizer.get_feature_names_out()\n",
    "print(len(cols))\n",
    "list(cols)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(), index=statements['text'].index, columns=cols)\n",
    "bar(df.mean().sort_values(ascending=False).head(30), horizontal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe the matrix of tdidf scores, we first perform a simple principal component analysis (`PCA`) with two modes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PCA(n_components=2).fit(np.log1p(X.toarray().T))\n",
    "df = pd.DataFrame(m.components_.T, index=statements.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PCA(n_components=2).fit(np.log1p(X.toarray().T))\n",
    "df = pd.DataFrame(m.components_.T, index=statements.index)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "years = [str(y) for y in df.index.year.unique()]\n",
    "colors = cm.RdBu(np.linspace(0, 1,len(years)))\n",
    "for i, y in enumerate(years): \n",
    "    ax.scatter(x=df.loc[y][0], y=df.loc[y][1], color=colors[i])\n",
    "ax.legend(years, loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "ax.set_xlabel(\"PC 0\")\n",
    "ax.set_ylabel(\"PC 1\")\n",
    "\n",
    "d = '2020-03-03'\n",
    "ax.text(x=df.loc[d][0], y=df.loc[d][1], s=d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two modes can be related to labor market and growth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: pd.concat([x.nlargest(), x.sort_values(ascending=False).tail(5)])\n",
    "W = pd.DataFrame(m.transform(np.log1p(X.toarray().T)), index=cols)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plt.subplots_adjust(wspace=.25)\n",
    "for i in [0, 1]: \n",
    "    bar(W[i].pipe(func), horizontal=True, ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning: document clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often information to group tokens into topics that explain differences across documents. A powerful algorithm is the non-negative matrix factorisation (`NMF`): for a non-negative matrix $X$ (such as the one with tfidf scores), `NMF` finds two other non-negative matrices such that: \n",
    "\n",
    "$$ X \\approx W H. $$ \n",
    "\n",
    "The number of topics (called `n_components` in the `scikit-learn` implementation) determines the number of columns in $W$ and the number of rows in $H$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 8\n",
    "m = NMF(n_components=n_components, \n",
    "        init='nndsvd', \n",
    "        solver='cd', \n",
    "        beta_loss='frobenius', \n",
    "        random_state=1, \n",
    "        alpha_W=0, \n",
    "        l1_ratio=0, \n",
    "        max_iter=500).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 2, figsize=(20, 16), sharex=True)\n",
    "ax = ax.ravel()\n",
    "for i in range(8):\n",
    "    bar(pd.Series(m.components_[i, :], cols)\\\n",
    "            .sort_values(ascending=False).head(10),\n",
    "    horizontal=True, ax=ax[i], title=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these topics interesting? This is a matter of interpretation, but at least, the graph below shows that these topics capture a strong element of time-clustering which makes it a bit less useful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pd.DataFrame(m.transform(X), index=df.index)\n",
    "line(W.resample('B').last().ffill(), cumsum=True, title='Cumulative topic loadings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: TFIDF + Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use the corpus of FOMC statements for supervised learning. More precisely, we match the text of the statements to the decision of the committee to raise rates, decrease rates or do nothing.  \n",
    "\n",
    "In practice, this implemented by using `scikit-learn pipelines` and chaining the `TfidfVectorizer` with a logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from ml4pmt.dataset import load_fomc_change_date\n",
    "fomc_change_up, fomc_change_dw = load_fomc_change_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomc_change_up, fomc_change_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = {'other_dt_change': ['2003-01-09', '2008-03-16', '2011-06-22'], \n",
    "         'statements_dt_change_other': ['2007-08-16'], \n",
    "         'qe1': ['2008-11-25', '2008-12-01', '2008-12-16', '2009-03-18'], \n",
    "        'qe2': ['2010-11-03'], \n",
    "         'twist': ['2011-09-21', '2012-06-20'],\n",
    "         'qe3':  ['2012-09-13', '2012-12-12', '2013-12-13'], \n",
    "         'corona': ['2020-03-20']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['2003-01-09', '2008-03-16', '2007-08-16', '2008-11-25', '2008-12-01', '2013-12-13', '2020-03-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = {'up': fomc_change_up, 'dw': fomc_change_dw, 'other': [d for c in other.values() for d in c]}\n",
    "dates['no change'] = statements.index.difference([d for c in dates.values() for d in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             min_df=5, max_df=.8, ngram_range=(1, 3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "X = vectorizer.fit_transform(statements['text'].values)\n",
    "\n",
    "m = PCA(n_components=2).fit(np.log1p(X.toarray().T))\n",
    "df = pd.DataFrame(m.components_.T, index=statements.index)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "colors = cm.RdBu(np.linspace(0, 1,len(dates)))\n",
    "for i, (k, v) in enumerate(dates.items()): \n",
    "    ax.scatter(x=df.loc[lambda x: x.index.intersection(v)][0], \n",
    "               y=df.loc[lambda x: x.index.intersection(v)][1], \n",
    "               color=colors[i])\n",
    "ax.legend(dates.keys(), loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "ax.set_xlabel(\"PC 0\")\n",
    "ax.set_ylabel(\"PC 1\")\n",
    "\n",
    "d = '2020-03-03'\n",
    "ax.text(x=df.loc[d][0], y=df.loc[d][1], s=d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from ml4pmt.text import coefs_plot, show_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline([('tfidf', TfidfVectorizer(vocabulary=None,\n",
    "                                          ngram_range=(1, 3),\n",
    "                                          max_features=500,\n",
    "                                          stop_words='english',\n",
    "                                          token_pattern=r'\\b[a-zA-Z]{3,}\\b')),\n",
    "                ('reg', LogisticRegression(C=1, l1_ratio=.35, \n",
    "                                           penalty='elasticnet', \n",
    "                                           solver='saga', max_iter=500)),\n",
    "               ])\n",
    "X, y = pd.concat([statements.loc[fomc_change_up].assign(change=1),\n",
    "                  statements.loc[fomc_change_dw].assign(change=-1)]).pipe(lambda df: (df['text'], df['change']))\n",
    "est.fit(X, y);\n",
    "vocab_ = pd.Series(est.named_steps['tfidf'].vocabulary_).sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_coef = pd.DataFrame(np.transpose(est.named_steps['reg'].coef_),index=vocab_)\n",
    "coefs_plot(interpret_coef, title='Interpreted coefficients for trained model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trick is that using a linear regression (e.g. ElasticNet) instead of a logistic regression is faster and as efficient (even sometimes better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline([\n",
    "('tfidf', TfidfVectorizer(vocabulary=None,\n",
    "                          ngram_range=(1, 3), \n",
    "                          max_features=500,\n",
    "                          stop_words='english',\n",
    "                          token_pattern=r'\\b[a-zA-Z]{3,}\\b')),\n",
    "    ('reg', ElasticNet(alpha=0.01)),])\n",
    "X, y = pd.concat([statements.loc[fomc_change_up].assign(change=1),\n",
    "                  statements.loc[fomc_change_dw].assign(change=-1)]).pipe(lambda df: (df['text'], df['change']))\n",
    "est.fit(X, y);\n",
    "vocab_ = pd.Series(est.named_steps['tfidf'].vocabulary_).sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_coef = pd.DataFrame(np.transpose(est.named_steps['reg'].coef_), index=vocab_)\n",
    "coefs_plot(interpret_coef, title='Interpreted coefficients for trained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "pred_tfidf = pd.Series(est.predict(statements['text']), index=statements.index).resample('B').last().ffill()\n",
    "line(pred_tfidf.rename('implied rate').to_frame()\\\n",
    "        .join(pd.Series(1, index=fomc_change_up).reindex(pred_tfidf.index).fillna(0).rename('up'))\\\n",
    "        .join(pd.Series(-1, index=fomc_change_dw).reindex(pred_tfidf.index).fillna(0).rename('dw')),\n",
    "sort=False, ax=ax, title='Implied interest rate (with forward information)')\n",
    "cols = ['corona', 'twist', 'qe1', 'qe2','qe3']\n",
    "for c in cols:\n",
    "    ax.plot(pred_tfidf.loc[other[c]], marker='*', ms=10)\n",
    "ax.legend(['implied rate', 'up', 'down'] + cols, loc='center left', bbox_to_anchor=(1, 0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexica = {'positive': interpret_coef.squeeze().nlargest(n=10),\n",
    "          'negative': interpret_coef.squeeze().nsmallest(n=10), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ = pd.Series(est.predict(X), index=X.index).sort_values().pipe(lambda x: [x.index[0], x.index[-1]])\n",
    "show_text(statements.loc[idx_], lexica=lexica, n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform Manifold Approximation and Projection (UMAP) is a popular dimension reduction technique (https://umap-learn.readthedocs.io/en/latest/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             min_df=5, max_df=.8, ngram_range=(1, 3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "X = vectorizer.fit_transform(statements['text'].values)\n",
    "\n",
    "Xtfm = UMAP().fit_transform(X)\n",
    "df = pd.DataFrame(Xtfm, index=statements.index)\n",
    "classes = df.index.year.unique()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "colors = cm.RdBu(np.linspace(0, 1, len(classes)))\n",
    "for i, y in enumerate(classes): \n",
    "    ax.scatter(df.loc[str(y)][0], df.loc[str(y)][1], color=colors[i])\n",
    "ax.legend(classes, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "d = '2020-03-03'\n",
    "ax.text(df.loc[d][0], df.loc[d][1], d, fontsize=9, rotation=0, ha='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Xtfm, index=statements.index)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "colors = cm.RdBu(np.linspace(0, 1,len(dates)))\n",
    "for i, (k, v) in enumerate(dates.items()): \n",
    "    ax.scatter(x=df.loc[lambda x: x.index.intersection(v)][0], \n",
    "               y=df.loc[lambda x: x.index.intersection(v)][1], \n",
    "               color=colors[i], alpha=.25)\n",
    "ax.legend(dates.keys(), loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "ax.set_xlabel(\"PC 0\")\n",
    "ax.set_ylabel(\"PC 1\")\n",
    "\n",
    "d = '2020-03-03'\n",
    "ax.text(x=df.loc[d][0], y=df.loc[d][1], s=d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "m = SentenceTransformer('all-distilroberta-v1', device='cpu')\n",
    "X = m.encode(statements['text'].values, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PCA(n_components=2).fit(np.log1p(X.T))\n",
    "df = pd.DataFrame(m.components_.T, index=statements.index)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "years = [str(y) for y in df.index.year.unique()]\n",
    "colors = cm.RdBu(np.linspace(0, 1,len(years)))\n",
    "for i, y in enumerate(years): \n",
    "    ax.scatter(x=df.loc[y][0], y=df.loc[y][1], color=colors[i])\n",
    "ax.legend(years, loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "ax.set_xlabel(\"PC 0\")\n",
    "ax.set_ylabel(\"PC 1\")\n",
    "\n",
    "d = '2020-03-03'\n",
    "ax.text(x=df.loc[d][0], y=df.loc[d][1], s=d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "Xtfm = UMAP().fit_transform(X)\n",
    "df_ = pd.DataFrame(Xtfm, index=statements.index)\n",
    "classes = df.index.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "colors = cm.RdBu(np.linspace(0, 1, len(classes)))\n",
    "for i, y in enumerate(classes): \n",
    "    ax.scatter(df_.loc[str(y)][0], df_.loc[str(y)][1], color=colors[i])\n",
    "ax.legend(classes, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "d = '2020-03-03'\n",
    "ax.text(df.loc[d][0], df.loc[d][1], d, fontsize=9, rotation=0, ha='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, index=statements.index)\n",
    "m = ElasticNet(alpha=0.01)\n",
    "X_, y_ = pd.concat([df.loc[fomc_change_up].assign(change=1),\n",
    "                  df.loc[fomc_change_dw].assign(change=-1)]).pipe(lambda df: (df.drop('change', axis=1), df['change']))\n",
    "m.fit(X_, y_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sbert = pd.Series(m.predict(df), index=statements.index).resample('B').last().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({'sbert': pred_sbert,  'tdfidf': pred_tfidf}, axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.concat({'sbert': pred_sbert,  'tdfidf': pred_tfidf}, axis=1).pipe(lambda x:x.div(x.std())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "ml4pmt",
   "language": "python",
   "name": "ml4pmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
