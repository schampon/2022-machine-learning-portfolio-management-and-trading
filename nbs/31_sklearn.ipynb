{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we take advantage of some of `scikit-learn` powerful features such as the `pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ml4pmt.plot import line, heatmap\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge / Lasso / Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression: the betas $\\langle \\beta_1, ..., \\beta_p \\rangle$ are chosen to minimize \n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "The Ridge regression provides more stable and accurate estimates than standard residual sum of squares minimization\n",
    "\n",
    "Lasso regression: the betas $\\langle \\beta_1,...,\\beta_p \\rangle$ are chosen to minimize \n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "The Lasso tends to promote sparse and stable models that can be more easily interpretable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net: the betas $\\langle \\beta_1,... ,\\beta_p \\rangle$ are chosen to minimize \n",
    "$$\n",
    "\t\\frac{1}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^p [(1-\\alpha) \\beta_j^2 + \\alpha |\\beta_j|]\n",
    "$$\n",
    "\n",
    ">  ``The lasso penalty is not very selective in the choice among a set of strong but correlated predictors, and the ridge penalty is inclined to shrink the coefficients of correlated variables towards each other. The compromise in the elastic net could cause the highly correlated features to be averaged while encouraging a parsiminuous model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an example, we use a diabetes dataset provided by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets\n",
    "X, y = datasets.load_diabetes(return_X_y=True,  as_frame=True)\n",
    "X /= X.std(axis=0)  \n",
    "eps = 5e-3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=eps)\n",
    "l1_ratio = 0.5\n",
    "alphas_enet, coefs_enet, _ = enet_path(X, y, eps=eps, l1_ratio=l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "fig.suptitle('Coefficients as a function of the shrinkage factor (in log)')\n",
    "line(pd.DataFrame(coefs_lasso.T, -1*np.log(alphas_lasso), columns=X.columns), title='Lasso', ax=ax[0])\n",
    "line(pd.DataFrame(coefs_enet.T, -1*np.log(alphas_enet), columns=X.columns), \n",
    "     title=f'Elastic net (l1_ratio={l1_ratio})', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more discussion in \n",
    "\n",
    "> Hastie, Trevor, et al. *The elements of statistical learning: data mining, inference, and prediction.* Vol. 2. New York: springer, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the industry backtest with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Industry momentum` section, we introduced a feature `X` as 12-month trailing past returns and target `y` as the one-month forward return. For $N$ industries (e.g. $N=12$), `X` and `y` are vectors of size $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are interested in fitting linear models (potentially with regularisation à la Ridge, Lasso or Elastic Net) such that \n",
    "\n",
    "$$ y = B X, $$ \n",
    "\n",
    "where $B$ is a matrix of size $[N \\times N]$. (In this equation, we ignore intercept terms which are generally small.) This equation falls into two lines of research: \n",
    "\n",
    "- in classic Econometrics, this is called a Vector Autoregressive (VAR) model (.e.g see the [wikipedia article](https://en.wikipedia.org/wiki/Vector_autoregression)). \n",
    "\n",
    "- in Machine-learning, this is often called a `multioutput` model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of this model in the context of industry timing is that it captures cross-industry effect: \n",
    "\n",
    "- for instance, a increasing trend on oil prices (which is positive for the Energy stocks) might be a negative predictor for sectors that use oil as input (e.g. Manufacturing)\n",
    "\n",
    "Such VAR/multioutput models will pick up the positive lead-lag correlations across industries and therefore potentially enrich the simple `Industry momentum` strategy that we introduced in the first section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Pipeline and Multi-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), the definition of a `sklearn` pipeline is: \n",
    "\n",
    "> Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator like linear regressions or `Ridge` regressions have the property to estimate with `multiouput` as the regression above, but are notheless estimators, not transormations. However, \n",
    "\n",
    "\n",
    "- in the pipelines that we are building, the last step is the `MeanVariance` class that produces the holdings;  \n",
    "\n",
    "- in the `sklearn` pipelines, all the steps except for the last one must be transformations; \n",
    "\n",
    "- despite formally not having a `transform` function, the `multioutput` linear estimators (such as `Ridge` and `Lasso`) are transformations of a vector `X` of size $N$ into a vector `y` of size $N$. \n",
    "\n",
    "In the following module, we extend the estimators that we will be using to have such `transform` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../ml4pmt/estimators.py\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression(LinearRegression):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "class Ridge(Ridge):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "class RidgeCV(RidgeCV):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "class MultiOutputRegressor(MultiOutputRegressor):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "class MLPRegressor(MLPRegressor):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data and the main custom functions to run a backtest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.backtesting import MeanVariance, Backtester\n",
    "from ml4pmt.dataset import load_kf_returns\n",
    "returns_data = load_kf_returns(cache_dir='data')\n",
    "\n",
    "ret = returns_data['Monthly']['Average_Value_Weighted_Returns'][:'1999']\n",
    "\n",
    "transform_X = lambda x: x.rolling(12).mean().fillna(0).values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference, we re-compute the pnl of a simple `Industry momentum` strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Backtester(MeanVariance(), ret).train(features, target)\n",
    "pnls = {'momentum':  m.pnl_ }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the functions that are specific to building `sklearn` pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.estimators import Ridge, LinearRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following simple pipeline, there are two steps: \n",
    "\n",
    "- the first step is a simple `multioutput` linear regression that produces fitted predictions for each industry\n",
    "\n",
    "- the second step takes these predictors and scales them as holdings with a mean-variance optimisation (with the `MeanVariance` class introduced earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(LinearRegression(), MeanVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Backtester(estimator, ret).train(features, target)\n",
    "pnls['linear_regression'] = m.pnl_ \n",
    "line(pnls['linear_regression'], cumsum=True, title='Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression fits an intercept and some coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_ = m.estimators_[0].named_steps['linearregression']\n",
    "coef_ = ols_.coef_\n",
    "intercept_ = ols_.intercept_\n",
    "vec = ret.mean().values\n",
    "np.allclose(ols_.predict(vec[None, :]), coef_.dot(vec)  + intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_ = [m.named_steps['linearregression'].coef_ for m in m.estimators_]\n",
    "coefs_mean = pd.DataFrame(sum(coefs_)/len(coefs_), ret.columns, ret.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(coefs_mean.loc[coefs_mean.mean(1).sort_values().index, coefs_mean.mean(1).sort_values().index],\n",
    "       title='Average linear regression coefficients (x-axis: predictors, y-axis=targets)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ = {}\n",
    "for hl in tqdm([6, 12, 24]): \n",
    "    features_ = ret.ewm(halflife=hl).mean().fillna(0).values\n",
    "    pnls_[hl] = Backtester(estimator, ret).train(features_, target).pnl_\n",
    "line(pnls_, cumsum=True, title='Robustness on feature half-lives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ = {}\n",
    "for hl in [6, 12, 24]: \n",
    "    features_ = ret.rolling(window=hl).mean().fillna(0).values\n",
    "    pnls_[hl] = Backtester(estimator, ret).train(features_, target).pnl_\n",
    "line(pnls_, cumsum=True, title='Robustness on features with rolling windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the `Ridge` regression applies a constraint across features, each feature needs to be properly rescaled, which is done here with `StandardScalar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          Ridge(), \n",
    "                          MeanVariance())\n",
    "\n",
    "pnls['ridge']= Backtester(estimator, ret).train(features, target).pnl_\n",
    "line(pnls['ridge'], cumsum=True, title='Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ = {}\n",
    "for alpha in [0.1, 1, 10, 100]: \n",
    "    estimator_ = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=alpha), MeanVariance())\n",
    "    pnls_[alpha] = Backtester(estimator_, ret).train(features_, target).pnl_\n",
    "line(pnls_, cumsum=True, title='Ridge: Robustness on alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge with feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand the set of features by using polynomial transfomrs with `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree=2).fit_transform(ret.iloc[:10]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of new features: intercept, initial features (=12), squared features (12), all cross features of degree 1 (=6*12): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          PolynomialFeatures(degree=2), \n",
    "                          Ridge(alpha=100), \n",
    "                          MeanVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of features generated by degree=2: {1+ 12 + 12 + 6 * 11}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls['ridge_with_feature_expansion'] = Backtester(estimator, ret).train(features_, target).pnl_\n",
    "line(pnls['ridge_with_feature_expansion'], cumsum=True, title='Ridge with feature extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ = {}\n",
    "for alpha in [0.1, 1, 10, 100, 1000]: \n",
    "    estimator_ = make_pipeline(StandardScaler(with_mean=False), \n",
    "                              PolynomialFeatures(degree=2), \n",
    "                              Ridge(alpha=alpha), \n",
    "                              MeanVariance())\n",
    "    pnls_[alpha] = Backtester(estimator_, ret).train(features_, target).pnl_\n",
    "line(pnls_, cumsum=True, title='Ridge with feature expansion: Robustness on alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all the types of linear predictors together, we can compare the cumulative pnls in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.concat(pnls, axis=1).assign(ALL = lambda x: x.mean(axis=1)), cumsum=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
