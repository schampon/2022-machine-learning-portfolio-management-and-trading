{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we construct a backtest using industry data. More precisely, we use data from Ken French's data library to construct a simple industry momentum return predictor.\n",
    "\n",
    "The goal of a backtest is to assess the validity of a trading predictor at any point in the past. In particular, it is crucial to avoid any forward-looking bias -- in which information available only after time $t$ is mistakingly used at time $t$. In practice, the predictors are estimated over `rolling` (or `expanding`) windows. We implement rolling window estimation with the `sklearn` `TimeSeriesSplit` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backtesting, visualisation is very important and we make use of some plotting functions introduced in the Appendix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.plot import line, bar, heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markowitz portfolio optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: a review of mean-variance optimisation for a universe with $N$ assets: $\\alpha$ is the return forecast: $\\alpha = E(r)$.\n",
    "\n",
    "\n",
    "**Lemma** [mean-variance]: the allocation that maximizes the utility $h^T \\alpha - \\frac{h^T V h}{2 \\lambda}$ is\n",
    "$$h = \\lambda V^{-1} \\alpha $$,  \n",
    "where $\\lambda$ is the risk-tolerance.\n",
    "\n",
    "The ex-ante risk is $h^T V h = \\lambda^2 \\alpha^T V^{-1} \\alpha$ and the ex-ante Sharpe ratio is\n",
    "$$\n",
    "S = \\frac{h^T E(r)}{\\sqrt{h^T V h}} = \\sqrt{\\alpha^T V^{-1} \\alpha}. \n",
    "$$\n",
    "\n",
    "**Corollary**: The maximisation of the sharpe ratio is equivalent (up to a scaling factor) the mean-variance optimisation. \n",
    "\n",
    "The mean-variance formula is extended to account for the linear constraints\n",
    "$$A h = b. $$ \n",
    "\n",
    "To do so, we introduce the Lagrangian $\\mathcal {L}$ (and Lagrange multiplier $\\xi$)\n",
    "\n",
    "$$\n",
    "\\mathcal {L}= h^T \\alpha - \\frac{h^T V h}{2\\lambda} - (h^T A^T - b^T)\\xi\n",
    "$$\n",
    "\n",
    "The Lagrange multiplier $\\xi$ is a `tuning parameter` chosen exactly so that the constraint above holds. At the optimal value of $\\xi$, the constrained problem boils down to an unconstrained problem with the adjusted return forecast $\\alpha - A^T \\xi$.\n",
    "\n",
    "\n",
    "**Lemma**: the allocation that maximizes the utility $h^T \\alpha - \\frac{h^T V h}{2 \\lambda}$ under the linear constraint $A h = b$ is\n",
    "\n",
    "$$ h = V^{-1} A^T \\left(A V^{-1} A^T \\right)^{-1} b + \\lambda V^{-1} \\left[\\alpha - A^T \\left(A V^{-1} A^T \\right)^{-1} A V^{-1} \\alpha \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*: the first-order condition is\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal {L}}{\\partial h} = \\alpha - \\frac{V h}{\\lambda} - A^T \\xi =0  \\Leftrightarrow  h = \\lambda V^{-1}[\\alpha - A^T \\xi] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\xi$ is chosen that $A h = b$\n",
    "\n",
    "$$b = Ah = \\lambda A  V^{-1}[\\alpha - A^T \\xi]  \\Rightarrow  \\xi = [A V^{-1}A^T]^{-1} \\left[ A  V^{-1}\\alpha - \\frac{b}{\\lambda}  \\right]\n",
    "$$\n",
    "\n",
    "The holding vector under constraint is\n",
    "\n",
    "$$ h_{\\lambda} = \\underbrace {V^{-1} A^T \\left(A V^{-1} A^T \\right)^{-1} b}_{\\text {minimum variance portfolio}} + \\underbrace { \\lambda V^{-1} \\left[\\alpha - A^T \\left(A V^{-1} A^T \\right)^{-1} A V^{-1} \\alpha \\right]}_{\\text {speculative portfolio}} $$\n",
    "\n",
    "- The first term is what minimises the risk $h^T V h$ under the constraint $Ah =b$ (in particular, it does not depend on expected returns or risk-tolerance).\n",
    "\n",
    "- The second term is the speculative portfolio (it is sensitive to both inputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficient frontier is the relation between  expected portfolio return $h^T \\alpha$ and portfolio standard deviation $\\sqrt{h^T V h}$ for varying level of risk-tolerance\n",
    "$$ (x, y) \\mapsto \\left(h_{\\lambda}^T \\alpha, \\sqrt{h_{\\lambda}^T V h_{\\lambda}} \\right)$$\n",
    "\n",
    "When $b=0$, the efficient frontier between $h_{\\lambda}^T \\alpha$ and $\\sqrt{h_{\\lambda}^T V h_{\\lambda}}$ is a line through $(0,0)$; otherwise, it is a parabolic curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on pure \"alpha views\" -- that is, long-short \"cash-neutral\" portfolios where the sum of holdings is zero. In this case $b=0$ and $A = \\textbf{1}$ where\n",
    "\n",
    "$$ \\textbf {1} = \\left[\\begin {array}{ccc} 1  & \\ldots & 1  \\end {array} \\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry momentum backtest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup for predicting industry returns is the following: \n",
    "\n",
    "- the assets are industries; \n",
    "\n",
    "- the return forecast $\\alpha$ is estimated using rolling-window returns (over $L$ months, $L=12$) preceding a given date; \n",
    "\n",
    "-  no `look-ahead bias`: at each date, only information up that date is used; \n",
    "\n",
    "- such a strategy goes long past \"winners\" (industries with higher-than-average returns) and goes short \"losers\" (industries with lower-than-average returns) $\\Rightarrow$ Momentum strategy; \n",
    "\n",
    "- this strategy is often implemented by skipping the most recent month to avoid the `1-month reversal\" effect`. \n",
    "\n",
    "See Moskowitz and Grinblatt (1999): \"Do Industries Explain Momentum,\"  *Journal of Finance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/l2_grinblatt_header.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "display(Image(\"images/l2_grinblatt_table3heading.PNG\"))\n",
    "display(Image(\"images/l2_grinblatt_table3heading2.PNG\"))\n",
    "display(Image(\"images/l2_grinblatt_table3.PNG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.dataset import load_kf_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_data = load_kf_returns(cache_dir=\"data\", force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Moskowitz-Grinblatt paper was published in August 1999, we will keep the data after 1999 as `out-of-sample` and only use the data before 1999. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = returns_data[\"Monthly\"][\"Average_Value_Weighted_Returns\"][:'1999']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time convention:\n",
    "- holdings $h_t$ and returns $r_t$ are known for period $t$ -- ie. *at the end of period $t$.\n",
    "\n",
    "- so to compute pnl with forward-looking information, the holdings must only depend on information up to $t-1$\n",
    "\n",
    "- in practice, we will have\n",
    "\n",
    "$$ pnl_t = h_{t-1} \\times r_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Backtesting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next set of helper file, we introduce three main functions: \n",
    "\n",
    "- a function that computes mean-variance holdings for batches\n",
    "\n",
    "- a `MeanVariance` class that follows the `sklearn` api\n",
    "\n",
    "- a `fit_and_predict` function to run rolling window estimations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%writefile ../ml4pmt/backtesting.py\n",
    "from sklearn.utils.metaestimators import _safe_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from ml4pmt.metrics import sharpe_ratio\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_batch_holdings(pred, V, A=None, past_h=None, constant_risk=False):\n",
    "    \"\"\"\n",
    "    compute markowitz holdings with return prediction \"mu\" and covariance matrix \"V\"\n",
    "\n",
    "    mu: numpy array (shape N * K)\n",
    "    V: numpy array (N * N)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    N, _ = V.shape\n",
    "    if isinstance(pred, pd.Series) | isinstance(pred, pd.DataFrame):\n",
    "        pred = pred.values\n",
    "    if pred.shape == (N,):\n",
    "        pred = pred[:, None]\n",
    "    elif pred.shape[1] == N:\n",
    "        pred = pred.T\n",
    "\n",
    "    invV = np.linalg.inv(V)\n",
    "    if A is None:\n",
    "        M = invV\n",
    "    else:\n",
    "        U = invV.dot(A)\n",
    "        if A.ndim == 1:\n",
    "            M = invV - np.outer(U, U.T) / U.dot(A)\n",
    "        else:\n",
    "            M = invV - U.dot(np.linalg.inv(U.T.dot(A)).dot(U.T))\n",
    "    h = M.dot(pred)\n",
    "    if constant_risk:\n",
    "        h = h / np.sqrt(np.diag(h.T.dot(V.dot(h))))\n",
    "    return h.T\n",
    "\n",
    "\n",
    "class MeanVariance(BaseEstimator):\n",
    "    def __init__(self, transform_V=None, A=None, constant_risk=True):\n",
    "\n",
    "        if transform_V is None:\n",
    "            self.transform_V = lambda x: np.cov(x.T)\n",
    "        else:\n",
    "            self.transform_V = transform_V\n",
    "        self.A = A\n",
    "        self.constant_risk = constant_risk\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.V_ = self.transform_V(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.A is None:\n",
    "            T, N = X.shape\n",
    "            A = np.ones(N)\n",
    "        else:\n",
    "            A = self.A\n",
    "        h = compute_batch_holdings(\n",
    "            X, self.V_, A, constant_risk=self.constant_risk)\n",
    "        return h\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return sharpe_ratio(np.sum(X * y, axis=1))\n",
    "\n",
    "\n",
    "class Backtester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator,\n",
    "        ret,\n",
    "        max_train_size=36,\n",
    "        test_size=1,\n",
    "        start_date=\"1945-01-01\",\n",
    "        end_date=None,\n",
    "    ):\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.estimator = estimator\n",
    "        self.ret = ret[: self.end_date]\n",
    "        self.cv = TimeSeriesSplit(\n",
    "            max_train_size=max_train_size,\n",
    "            test_size=test_size,\n",
    "            n_splits=1 + len(ret.loc[start_date:end_date]) // test_size,\n",
    "        )\n",
    "\n",
    "    def train(self, features, target):\n",
    "        pred, estimators = fit_predict(\n",
    "            self.estimator, features, target, self.ret, self.cv, return_estimator=True\n",
    "        )\n",
    "        self.estimators_ = estimators\n",
    "        self.h_ = pred\n",
    "        if isinstance(pred, pd.DataFrame): \n",
    "            self.pnl_ = (\n",
    "                pred.shift(1).mul(self.ret).sum(axis=1)[\n",
    "                    self.start_date: self.end_date]\n",
    "            )\n",
    "        elif isinstance(pred, pd.Series):\n",
    "            self.pnl_ = (\n",
    "                pred.shift(1).mul(self.ret)[\n",
    "                    self.start_date: self.end_date]\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n",
    "def _fit_predict(estimator, X, y, train, test, return_estimator=False):\n",
    "    X_train, y_train = _safe_split(estimator, X, y, train)\n",
    "    X_test, _ = _safe_split(estimator, X, y, test, train)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    if return_estimator:\n",
    "        return estimator.predict(X_test), estimator\n",
    "    else:\n",
    "        return estimator.predict(X_test)\n",
    "\n",
    "\n",
    "def fit_predict(\n",
    "    estimator,\n",
    "    features,\n",
    "    target,\n",
    "    ret,\n",
    "    cv,\n",
    "    return_estimator=False,\n",
    "    verbose=0,\n",
    "    pre_dispatch=\"2*n_jobs\",\n",
    "    n_jobs=1,\n",
    "):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n",
    "                        pre_dispatch=pre_dispatch)\n",
    "    res = parallel(\n",
    "        delayed(_fit_predict)(\n",
    "            clone(estimator), features, target, train, test, return_estimator\n",
    "        )\n",
    "        for train, test in cv.split(ret)\n",
    "    )\n",
    "    if return_estimator:\n",
    "        pred, estimators = zip(*res)\n",
    "    else:\n",
    "        pred = res\n",
    "\n",
    "    idx = ret.index[np.concatenate([test for _, test in cv.split(ret)])]\n",
    "    if isinstance(ret, pd.DataFrame): \n",
    "        cols = ret.columns\n",
    "        df = pd.DataFrame(np.concatenate(pred), index=idx, columns=cols)\n",
    "    elif isinstance(ret, pd.Series): \n",
    "        df = pd.Series(np.concatenate(pred), index=idx)\n",
    "    else: \n",
    "        df = None \n",
    "\n",
    "    if return_estimator:\n",
    "        return df, estimators\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.backtesting import compute_batch_holdings, MeanVariance, Backtester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, N = ret.shape\n",
    "A = np.ones(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = compute_batch_holdings(ret.mean(), ret.cov(), A, past_h=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(h.dot(A), [0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.stack([np.ones(N), np.zeros(N)], axis=1)\n",
    "A[0, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = compute_batch_holdings(pred=ret.mean(), V=ret.cov(), A=A, past_h=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(h.dot(A), [0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the data is monthly, we re-estimate the model every month. This is done by choosing the parameter `n_splits` in the class `TimeSeriesSplit` as the number of months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"1945-01-01\"\n",
    "test_size = 1\n",
    "params = dict(max_train_size=36, test_size=test_size, gap=0)\n",
    "params[\"n_splits\"] = 1 + len(ret.loc[start_date:]) // test_size\n",
    "\n",
    "cv = TimeSeriesSplit(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, with `TimeSeriesSplit`:\n",
    "\n",
    "- the `test` indices are the dates for which the holdings are computed.\n",
    "\n",
    "- the `train` indices are the date range over which a forecasting model is trained.\n",
    "\n",
    "- the target will been shifted by $-1$ and `gap` is set to 0. \n",
    "\n",
    "- we can estimate batches with `test_size` > 1.\n",
    "\n",
    "- `n_splits` is determined so that the backtest starts (just) before a certain start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv.split(ret):\n",
    "    break\n",
    "ret.iloc[train].index[-1], ret.iloc[test].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Empiricial results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Cumulative pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_X = lambda x: x.rolling(12).mean().values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_h = []\n",
    "for train, test in cv.split(ret): \n",
    "    m = MeanVariance()\n",
    "    m.fit(features[train], target[train])\n",
    "    _h += [m.predict(features[test])]\n",
    "    \n",
    "cols = ret.columns \n",
    "idx = ret.index[np.concatenate([test for _, test in cv.split(ret)])]\n",
    "h = pd.DataFrame(np.concatenate(_h), index=idx, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = h.shift(1).mul(ret).sum(axis=1)[start_date:]\n",
    "line(pnl.rename('Industry momentum'), cumsum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Backtester(estimator=MeanVariance(), ret=ret)\n",
    "m.train(features, target)\n",
    "h.equals(m.h_), pnl.equals(m.pnl_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Other backtest statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract information for the estimator -- e.g. in this simple case, recover the covariance matrix fitted by the class `MeanVariance()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = m.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_mean = pd.DataFrame(sum([m.V_ for m in estimators])/len(estimators), ret.columns, ret.columns)\n",
    "heatmap(V_mean, title='Average covariance matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Backtester(estimator=MeanVariance(), ret=ret)\n",
    "pnls = {}\n",
    "for window in [6, 12, 24, 36]: \n",
    "    features_ = ret.rolling(window).mean().values\n",
    "    m.train(features_, target)\n",
    "    pnls[window] = m.pnl_\n",
    "line(pnls, cumsum=True, start_date='1945', title='Cumulative pnl for different look-back windows (in month)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.metrics import sharpe_ratio\n",
    "sr = {i: h.shift(1+i).mul(ret).sum(axis=1).pipe(sharpe_ratio) for i in range(-10, 12)}\n",
    "bar(sr, baseline=0, sort=False, title='Lead-lag sharpe ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `off-the-top` approach is to remove an asset from the tradable set and check whether the portfolio sharpe ratio decreases (in which case, this asset is a *contributor*) or increases (in which case, this asset is a *detractor*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ott = {}\n",
    "for c in ret.columns:\n",
    "    ret_ = ret.drop(c, axis=1)\n",
    "    features_ = transform_X(ret_)\n",
    "    target_ = transform_y(ret_)\n",
    "    pnl_ = Backtester(estimator=MeanVariance(), ret=ret_).train(features_, target_).pnl_\n",
    "    pnls_ott[c] = pnl_.pipe(sharpe_ratio)\n",
    "\n",
    "pnls_ott[\"ALL\"] = pnl.pipe(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(pnls_ott, baseline=\"ALL\", title='Industry momentum off-the-top')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "ml4pmt",
   "language": "python",
   "name": "ml4pmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
