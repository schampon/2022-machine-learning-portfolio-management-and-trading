{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create some helper functions that will be used across notebooks using the magic `%%writefile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../ml4pmt/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_monthly(df):\n",
    "    return int(len(df) / len(df.asfreq(\"M\"))) == 1\n",
    "\n",
    "\n",
    "def test_bday(df):\n",
    "    return int(len(df) / len(df.asfreq(\"B\"))) == 1\n",
    "\n",
    "\n",
    "def test_day(df):\n",
    "    return int(len(df) / len(df.asfreq(\"D\"))) == 1\n",
    "\n",
    "\n",
    "def sharpe_ratio(df, num_period_per_year=None):\n",
    "    num_period_per_year = None\n",
    "    if test_monthly(df):\n",
    "        num_period_per_year = 12\n",
    "    if test_bday(df):\n",
    "        num_period_per_year = 260\n",
    "    if test_day(df):\n",
    "        num_period_per_year = 365\n",
    "    if num_period_per_year is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return df.mean() / df.std() * np.sqrt(num_period_per_year)\n",
    "\n",
    "\n",
    "def drawdown(x, return_in_risk_unit=True, window=36, num_period_per_year=12):\n",
    "    dd = x.cumsum().sub(x.cumsum().cummax())\n",
    "    if return_in_risk_unit:\n",
    "        return dd.div(x.rolling(window).std().mul(np.sqrt(num_period_per_year)))\n",
    "    else:\n",
    "        return dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration, in particular based on visualisation, is crucial to modern data science. `Pandas` has a lot of plotting functionalities (e.g. see the graph below), but we will find it usefull to use a custom `plot` set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%writefile ../ml4pmt/plot.py\n",
    "from ml4pmt.metrics import sharpe_ratio\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "\n",
    "def line(\n",
    "    df,\n",
    "    sort=True,\n",
    "    figsize=(8, 5),\n",
    "    ax=None,\n",
    "    title=\"\",\n",
    "    cumsum=False,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    legend_sharpe_ratio=None,\n",
    "    legend=True,\n",
    "    yscale=None,\n",
    "    start_date=None,\n",
    "):\n",
    "    if loc == \"best\":\n",
    "        bbox_to_anchor = None\n",
    "    if isinstance(df, dict):\n",
    "        df = pd.concat(df, axis=1)\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    if start_date is not None:\n",
    "        df = df[start_date:]\n",
    "    if cumsum & (legend_sharpe_ratio is None):\n",
    "        legend_sharpe_ratio = True\n",
    "    if legend_sharpe_ratio:\n",
    "        df.columns = [\n",
    "            f\"{c}: sr={sharpe_ratio(df[c]): 3.2f}\" for c in df.columns]\n",
    "    if cumsum:\n",
    "        df = df.cumsum()\n",
    "    if sort:\n",
    "        df = df.loc[:, lambda x: x.iloc[-1].sort_values(ascending=False).index]\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.plot(df.index, df.values)\n",
    "    if legend:\n",
    "        ax.legend(df.columns, loc=loc, bbox_to_anchor=bbox_to_anchor)\n",
    "    ax.set_title(title)\n",
    "    if yscale == \"log\":\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "def bar(\n",
    "    df,\n",
    "    err=None,\n",
    "    sort=True,\n",
    "    figsize=(8, 5),\n",
    "    ax=None,\n",
    "    title=\"\",\n",
    "    horizontal=False,\n",
    "    baseline=None,\n",
    "    rotation=0,\n",
    "):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df.squeeze()\n",
    "    if isinstance(df, dict):\n",
    "        df = pd.Series(df)\n",
    "    if sort:\n",
    "        df = df.sort_values()\n",
    "    if err is not None:\n",
    "        err = err.loc[df.index]\n",
    "    labels = df.index\n",
    "    x = np.arange(len(labels))\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    if horizontal:\n",
    "        ax.barh(x, df.values, xerr=err, capsize=5)\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_yticklabels(labels, rotation=0)\n",
    "        if baseline in df.index:\n",
    "            df_ = df.copy()\n",
    "            df_[df.index != baseline] = 0\n",
    "            ax.barh(x, df_.values, color=\"lightgreen\")\n",
    "    else:\n",
    "        ax.bar(x, df.values, yerr=err, capsize=5)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=0)\n",
    "        if baseline in df.index:\n",
    "            df_ = df.copy()\n",
    "            df_[df.index != baseline] = 0\n",
    "            ax.bar(x, df_.values, color=\"lightgreen\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def heatmap(\n",
    "    df,\n",
    "    ax=None,\n",
    "    figsize=(8, 5),\n",
    "    title=\"\",\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    vcompute=True,\n",
    "    cmap=\"RdBu\",\n",
    "):\n",
    "    labels_x = df.index\n",
    "    x = np.arange(len(labels_x))\n",
    "    labels_y = df.columns\n",
    "    y = np.arange(len(labels_y))\n",
    "    if vcompute:\n",
    "        vmax = df.abs().max().max()\n",
    "        vmin = -vmax\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    pos = ax.imshow(\n",
    "        df.T.values, cmap=cmap, interpolation=\"nearest\", vmax=vmax, vmin=vmin\n",
    "    )\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_xticklabels(labels_x, rotation=90)\n",
    "    ax.set_yticklabels(labels_y)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    fig.colorbar(pos, ax=ax)\n",
    "\n",
    "\n",
    "def scatter(\n",
    "    df,\n",
    "    xscale=None,\n",
    "    yscale=None,\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    xticks=None,\n",
    "    yticks=None,\n",
    "    figsize=(8, 5),\n",
    "    title=None,\n",
    "):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.scatter(df, df.index, facecolors=\"none\", edgecolors=\"b\", s=50)\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    if xscale is not None:\n",
    "        ax.set_xscale(xscale)\n",
    "    if yscale is not None:\n",
    "        ax.set_yscale(yscale)\n",
    "    if yticks is not None:\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(yticks)\n",
    "    if xticks is not None:\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticks)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.plot import bar, line, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.Series(np.random.normal(size=50)), cumsum=True, title=\"This is a graph\", \n",
    "     legend_sharpe_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(pd.Series(np.random.normal(size=50)), baseline=10, horizontal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we look at two datasets:\n",
    "\n",
    "- Ken French's data library (https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html).\n",
    "\n",
    "- Berkshire Hathaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     104,
     131,
     191,
     225,
     236
    ]
   },
   "outputs": [],
   "source": [
    "%%writefile ../ml4pmt/dataset.py\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "import gdown\n",
    "import subprocess\n",
    "\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def clean_kf_dataframes(df, multi_df=False):\n",
    "    \"\"\"\n",
    "    extract the annual and monthly dataframes from the csv file with specific formatting\n",
    "    \"\"\"\n",
    "    idx = [-2] + list(np.where(df.notna().sum(axis=1) == 0)[0])\n",
    "    if multi_df:\n",
    "        cols = [\"  Average Value Weighted Returns -- Monthly\"] + list(\n",
    "            df.loc[df.notna().sum(axis=1) == 0].index\n",
    "        )\n",
    "    returns_data = {\"Annual\": {}, \"Monthly\": {}}\n",
    "    for i in range(len(idx)):\n",
    "        if multi_df:\n",
    "            c_ = (\n",
    "                cols[i]\n",
    "                .replace(\"-- Annual\", \"\")\n",
    "                .replace(\"-- Monthly\", \"\")\n",
    "                .strip()\n",
    "                .replace(\"/\", \" \")\n",
    "                .replace(\" \", \"_\")\n",
    "            )\n",
    "        if i != len(idx) - 1:\n",
    "            v = df.iloc[idx[i] + 2: idx[i + 1] - 1].astype(float)\n",
    "            v.index = v.index.str.strip()\n",
    "            if len(v) != 0:\n",
    "                if len(v.index[0]) == 6:\n",
    "                    v.index = pd.to_datetime(v.index, format=\"%Y%m\")\n",
    "                    if multi_df:\n",
    "                        returns_data[\"Monthly\"][c_] = v\n",
    "                    else:\n",
    "                        returns_data[\"Monthly\"] = v\n",
    "                    continue\n",
    "                if len(v.index[0]) == 4:\n",
    "                    v.index = pd.to_datetime(v.index, format=\"%Y\")\n",
    "                    if multi_df:\n",
    "                        returns_data[\"Annual\"][c_] = v\n",
    "                    else:\n",
    "                        returns_data[\"Annual\"] = v\n",
    "    return returns_data\n",
    "\n",
    "\n",
    "def load_kf_returns(\n",
    "    filename=\"12_Industry_Portfolios\", cache_dir=None, force_reload=False\n",
    "):\n",
    "    \"\"\"\n",
    "    load industry returns for Ken French website:\n",
    "    https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "    \"\"\"\n",
    "\n",
    "    if filename == \"12_Industry_Portfolios\":\n",
    "        skiprows, multi_df = 11, True\n",
    "    if filename == \"F-F_Research_Data_Factors\":\n",
    "        skiprows, multi_df = 3, False\n",
    "    if filename == \"F-F_Momentum_Factor\":\n",
    "        skiprows, multi_df = 13, False\n",
    "    if filename == \"F-F_Research_Data_Factors_daily\":\n",
    "        skiprows, multi_df = 4, False\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "    output_dir = cache_dir / filename\n",
    "    if (output_dir.is_dir()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache directory: {output_dir}\")\n",
    "        returns_data = load_dict(output_dir)\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        path = (\n",
    "            \"http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/\"\n",
    "            + filename\n",
    "            + \"_CSV.zip\"\n",
    "        )\n",
    "        r = requests.get(path)\n",
    "        files = ZipFile(BytesIO(r.content))\n",
    "\n",
    "        df = pd.read_csv(files.open(filename + \".CSV\"),\n",
    "                         skiprows=skiprows, index_col=0)\n",
    "        if 'daily' in filename: \n",
    "            returns_data = {'Daily': df.iloc[:-1].pipe(lambda x: x.set_index(pd.to_datetime(x.index)))}            \n",
    "        else: \n",
    "            returns_data = clean_kf_dataframes(df, multi_df=multi_df)\n",
    "\n",
    "\n",
    "        logger.info(f\"saving in cache directory {output_dir}\")\n",
    "        save_dict(returns_data, output_dir)\n",
    "    return returns_data\n",
    "\n",
    "\n",
    "def load_buffets_data(cache_dir=None, force_reload=False):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "\n",
    "    filename = cache_dir / \"ffdata_brk13f.parquet\"\n",
    "\n",
    "    if (filename.is_file()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache directory: {filename}\")\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        path = \"https://github.com/slihn/buffetts_alpha_R/archive/master.zip\"\n",
    "        r = requests.get(path)\n",
    "        files = ZipFile(BytesIO(r.content))\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            files.open(\"buffetts_alpha_R-master/ffdata_brk13f.csv\"), index_col=0\n",
    "        )\n",
    "        df.index = pd.to_datetime(df.index, format=\"%m/%d/%Y\")\n",
    "        logger.info(f\"saving in cache directory {filename}\")\n",
    "        df.to_parquet(filename)\n",
    "    return df\n",
    "\n",
    "\n",
    "symbol_dict = {\n",
    "    \"TOT\": \"Total\",\n",
    "    \"XOM\": \"Exxon\",\n",
    "    \"CVX\": \"Chevron\",\n",
    "    \"COP\": \"ConocoPhillips\",\n",
    "    \"VLO\": \"Valero Energy\",\n",
    "    \"MSFT\": \"Microsoft\",\n",
    "    \"IBM\": \"IBM\",\n",
    "    \"TWX\": \"Time Warner\",\n",
    "    \"CMCSA\": \"Comcast\",\n",
    "    \"CVC\": \"Cablevision\",\n",
    "    \"YHOO\": \"Yahoo\",\n",
    "    \"DELL\": \"Dell\",\n",
    "    \"HPQ\": \"HP\",\n",
    "    \"AMZN\": \"Amazon\",\n",
    "    \"TM\": \"Toyota\",\n",
    "    \"CAJ\": \"Canon\",\n",
    "    \"SNE\": \"Sony\",\n",
    "    \"F\": \"Ford\",\n",
    "    \"HMC\": \"Honda\",\n",
    "    \"NAV\": \"Navistar\",\n",
    "    \"NOC\": \"Northrop Grumman\",\n",
    "    \"BA\": \"Boeing\",\n",
    "    \"KO\": \"Coca Cola\",\n",
    "    \"MMM\": \"3M\",\n",
    "    \"MCD\": \"McDonald's\",\n",
    "    \"PEP\": \"Pepsi\",\n",
    "    \"K\": \"Kellogg\",\n",
    "    \"UN\": \"Unilever\",\n",
    "    \"MAR\": \"Marriott\",\n",
    "    \"PG\": \"Procter Gamble\",\n",
    "    \"CL\": \"Colgate-Palmolive\",\n",
    "    \"GE\": \"General Electrics\",\n",
    "    \"WFC\": \"Wells Fargo\",\n",
    "    \"JPM\": \"JPMorgan Chase\",\n",
    "    \"AIG\": \"AIG\",\n",
    "    \"AXP\": \"American express\",\n",
    "    \"BAC\": \"Bank of America\",\n",
    "    \"GS\": \"Goldman Sachs\",\n",
    "    \"AAPL\": \"Apple\",\n",
    "    \"SAP\": \"SAP\",\n",
    "    \"CSCO\": \"Cisco\",\n",
    "    \"TXN\": \"Texas Instruments\",\n",
    "    \"XRX\": \"Xerox\",\n",
    "    \"WMT\": \"Wal-Mart\",\n",
    "    \"HD\": \"Home Depot\",\n",
    "    \"GSK\": \"GlaxoSmithKline\",\n",
    "    \"PFE\": \"Pfizer\",\n",
    "    \"SNY\": \"Sanofi-Aventis\",\n",
    "    \"NVS\": \"Novartis\",\n",
    "    \"KMB\": \"Kimberly-Clark\",\n",
    "    \"R\": \"Ryder\",\n",
    "    \"GD\": \"General Dynamics\",\n",
    "    \"RTN\": \"Raytheon\",\n",
    "    \"CVS\": \"CVS\",\n",
    "    \"CAT\": \"Caterpillar\",\n",
    "    \"DD\": \"DuPont de Nemours\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_sklearn_stock_returns(cache_dir=None, force_reload=False):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "\n",
    "    filename = cache_dir / \"sklearn_returns.parquet\"\n",
    "\n",
    "    if (filename.is_file()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache directory: {filename}\")\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        url = \"https://raw.githubusercontent.com/scikit-learn/examples-data/master/financial-data\"\n",
    "        df = (\n",
    "            pd.concat(\n",
    "                {\n",
    "                    c: pd.read_csv(f\"{url}/{c}.csv\", index_col=0, parse_dates=True)[\n",
    "                        \"close\"\n",
    "                    ].diff()\n",
    "                    for c in symbol_dict.keys()\n",
    "                },\n",
    "                axis=1,\n",
    "            )\n",
    "            .asfreq(\"B\")\n",
    "            .iloc[1:]\n",
    "        )\n",
    "\n",
    "        logger.info(f\"saving in cache directory {filename}\")\n",
    "        df.to_parquet(filename)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_dict(data, output_dir):\n",
    "    assert isinstance(data, dict)\n",
    "    if output_dir.is_dir() is False:\n",
    "        os.mkdir(output_dir)\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            v.to_parquet(output_dir / f\"{k}.parquet\")\n",
    "        else:\n",
    "            save_dict(v, output_dir=output_dir / k)\n",
    "\n",
    "\n",
    "def load_dict(input_dir):\n",
    "    data = {}\n",
    "    for o in os.scandir(input_dir):\n",
    "        if o.name.endswith(\".parquet\"):\n",
    "            k = o.name.replace(\".parquet\", \"\")\n",
    "            data[k] = pd.read_parquet(o)\n",
    "        elif o.is_dir:\n",
    "            data[o.name] = load_dict(o)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def get_fomc_urls(from_year = 1999, switch_year=2017): \n",
    "    calendar_url = 'https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm'\n",
    "    r = requests.get(calendar_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    contents = soup.find_all('a', href=re.compile('^/newsevents/pressreleases/monetary\\d{8}[ax].htm'))\n",
    "    urls_ = [content.attrs['href'] for content in contents]\n",
    "\n",
    "    for year in range(from_year, switch_year):\n",
    "        yearly_contents = []\n",
    "        fomc_yearly_url = f'https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm'\n",
    "        r_year = requests.get(fomc_yearly_url)\n",
    "        soup_yearly = BeautifulSoup(r_year.text, 'html.parser')\n",
    "        yearly_contents = soup_yearly.findAll('a', text='Statement')\n",
    "        for yearly_content in yearly_contents:\n",
    "            urls_.append(yearly_content.attrs['href'])\n",
    "    \n",
    "    urls = ['https://www.federalreserve.gov' + url for url in urls_]\n",
    "    return urls \n",
    "\n",
    "def sent_cleaner(s): \n",
    "    return s.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip() \n",
    "    \n",
    "def bs_cleaner(bs, html_tag_blocked=None): \n",
    "    if html_tag_blocked is None: \n",
    "        html_tag_blocked = ['style', 'script', '[document]',  'meta',  'a',  'span',  'label', 'strong', 'button', \n",
    "                     'li', 'h6',  'font', 'h1', 'h2',  'h3', 'h5', 'h4',  'em', 'body', 'head']\n",
    "    return [sent_cleaner(t) for t in bs.find_all(text=True) \n",
    "            if (t.parent.name not in html_tag_blocked)&(len(sent_cleaner(t))>0)]\n",
    "\n",
    "\n",
    "\n",
    "regexp = re.compile(r'\\s+', re.UNICODE)\n",
    "\n",
    "def feature_extraction(corpus, sent_filters=None):\n",
    "    if sent_filters is None: \n",
    "        sent_filters = ['Board of Governors', 'Federal Reserve System',\n",
    "                         '20th Street and Constitution Avenue N.W., Washington, DC 20551',\n",
    "                         'Federal Reserve Board - Federal Reserve issues FOMC statement',\n",
    "                         'For immediate release',\n",
    "                         'Federal Reserve Board - FOMC statement',\n",
    "                         'DO NOT REMOVE:  Wireless Generation',\n",
    "                         'For media inquiries', \n",
    "                         'or call 202-452-2955.',\n",
    "                         'Voting', \n",
    "                         'For release at', \n",
    "                         'For immediate release', \n",
    "                        'Last Update', \n",
    "                        'Last update'\n",
    "                        ]\n",
    "\n",
    "    text = [' '.join([regexp.sub(' ', s) for i, s in enumerate(c) \n",
    "                     if (i>1) & np.all([q not in s for q in sent_filters])])\n",
    "            for c in corpus]\n",
    "    \n",
    "    release_date = [pd.to_datetime(c[1].replace('Release Date: ', '')) for c in corpus]\n",
    "    last_update = [pd.to_datetime([s.replace('Last update:', '').replace('Last Update:', '').strip() \n",
    "                   for s in c if 'last update: ' in s.lower()][0]) for c in corpus]\n",
    "    voting = [' '.join([s for s in c if 'Voting' in s]) for c in corpus]\n",
    "    release_time = [' '.join([s for s in c if ('For release at' in s)|('For immediate release' in s)]) for c in corpus]\n",
    "    \n",
    "    return pd.DataFrame({'release_date': release_date, \n",
    "                         'last_update':  last_update, \n",
    "                         'text': text, \n",
    "                         'voting': voting, \n",
    "                         'release_time': release_time})\n",
    "\n",
    "def load_fomc_statements(add_url=True, cache_dir=None, force_reload=False, \n",
    "                        progress_bar=True, from_year=1999):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "        \n",
    "    filename = cache_dir / 'fomc_statements.parquet'\n",
    "    if (filename.exists()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache file: {filename}\")\n",
    "        statements = pd.read_parquet(filename)\n",
    "    else: \n",
    "        logger.info(\"loading from external source\")\n",
    "        urls = get_fomc_urls(from_year = from_year)\n",
    "        if progress_bar: \n",
    "            urls_ = tqdm(urls)\n",
    "        else: \n",
    "            urls_ = urls \n",
    "        corpus = [bs_cleaner(BeautifulSoup(requests.get(url).text, 'html.parser')) for url in urls_]\n",
    "        statements = feature_extraction(corpus).set_index('release_date')\n",
    "        if add_url: \n",
    "            statements = statements.assign(url=urls)\n",
    "        statements= statements.sort_index()\n",
    "        logger.info(f\"saving cache file {filename}\")\n",
    "        statements.to_parquet(filename)\n",
    "    return statements\n",
    "\n",
    "\n",
    "def load_fomc_change_date(as_datetime=True): \n",
    "    change_up = pd.to_datetime(['1999-06-30', '1999-08-24', '1999-11-16', '2000-02-02',\n",
    "                                          '2000-03-21', '2000-05-16', '2004-06-30', '2004-08-10',\n",
    "                                          '2004-09-21', '2004-11-10', '2004-12-14', '2005-02-02',\n",
    "                                          '2005-03-22', '2005-05-03', '2005-06-30', '2005-08-09',\n",
    "                                          '2005-09-20', '2005-11-01', '2005-12-13', '2006-01-31',\n",
    "                                          '2006-03-28', '2006-05-10', '2006-06-29', '2015-12-16',\n",
    "                                          '2016-12-14', '2017-03-15', '2017-06-14', '2017-12-13',\n",
    "                                          '2018-03-21', '2018-06-13', '2018-09-26', '2018-12-19',\n",
    "                                          '2022-03-16', '2022-05-04', '2022-06-15',\n",
    "                                          '2022-07-27'])\n",
    "\n",
    "    change_dw = pd.to_datetime(['2001-01-03', '2001-01-31', '2001-03-20', '2001-04-18',\n",
    "                                          '2001-05-15', '2001-06-27', '2001-08-21', '2001-09-17',\n",
    "                                          '2001-10-02', '2001-11-06', '2001-12-11', '2002-11-06',\n",
    "                                          '2003-06-25', '2007-09-18', '2007-10-31', '2007-12-11',\n",
    "                                          '2008-01-22', '2008-01-30', '2008-03-18', '2008-04-30',\n",
    "                                          '2008-10-08', '2008-10-29', '2008-12-16',\n",
    "                                          '2019-07-31', '2019-09-18', '2019-10-30',\n",
    "                                          '2020-03-03', '2020-03-15'])\n",
    "    if as_datetime: \n",
    "        change_up, change_dw = pd.to_datetime(change_up), pd.to_datetime(change_dw)\n",
    "\n",
    "    return change_up, change_dw\n",
    "\n",
    "\n",
    "def load_loughran_mcdonald_dictionary(cache_dir=None, force_reload=False, quiet=True): \n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "        \n",
    "    filename = cache_dir / 'Loughran-McDonald_MasterDictionary_1993-2021.csv'\n",
    "    if (filename.exists()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache file: {filename}\")\n",
    "    else: \n",
    "        logger.info(\"loading from external source\")\n",
    "        url = 'https://drive.google.com/uc?id=17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik'\n",
    "        output = str(filename)\n",
    "        gdown.download(url, output, quiet=quiet, fuzzy=True)\n",
    "        \n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "\n",
    "mapping_10X =   {'AAPL': ['APPLE COMPUTER INC', 'APPLE INC'],\n",
    "             'AIG': 'AMERICAN INTERNATIONAL GROUP INC',\n",
    "             'AMZN': 'AMAZON COM INC',\n",
    "             'AXP': 'AMERICAN EXPRESS CO',\n",
    "             'BA': 'BOEING CO',\n",
    "             'BAC': 'BANK OF AMERICA CORP /DE/',\n",
    "             'CAT': 'CATERPILLAR INC',\n",
    "             'CL': 'COLGATE PALMOLIVE CO',\n",
    "             'CMCSA': 'COMCAST CORP',\n",
    "             'COP': 'CONOCOPHILLIPS',\n",
    "             'CSCO': 'CISCO SYSTEMS INC',\n",
    "             'CVC': 'CABLEVISION SYSTEMS CORP /NY',\n",
    "             'CVS': ['CVS CORP', 'CVS/CAREMARK CORP', 'CVS CAREMARK CORP'],\n",
    "             'CVX': ['CHEVRONTEXACO CORP', 'CHEVRON CORP'],\n",
    "             'DD': 'DUPONT E I DE NEMOURS & CO',\n",
    "             'DELL': ['DELL COMPUTER CORP', 'DELL INC'],\n",
    "             'F': 'FORD MOTOR CO',\n",
    "             'GD': 'GENERAL DYNAMICS CORP',\n",
    "             'GE': 'GENERAL ELECTRIC CO',\n",
    "             'GS': 'GOLDMAN SACHS GROUP INC/',\n",
    "             'HD': 'HOME DEPOT INC',\n",
    "             'HPQ': 'HEWLETT PACKARD CO',\n",
    "             'IBM': 'INTERNATIONAL BUSINESS MACHINES CORP',\n",
    "             'JPM': 'J P MORGAN CHASE & CO',\n",
    "             'K': 'KELLOGG CO',\n",
    "             'KMB': 'KIMBERLY CLARK CORP',\n",
    "             'KO': 'COCA COLA CO',\n",
    "             'MAR': 'MARRIOTT INTERNATIONAL INC /MD/',\n",
    "             'MCD': 'MCDONALDS CORP',\n",
    "             'MMM': '3M CO',\n",
    "             'MSFT': 'MICROSOFT CORP',\n",
    "             'NAV': 'NAVISTAR INTERNATIONAL CORP',\n",
    "             'NOC': 'NORTHROP GRUMMAN CORP /DE/',\n",
    "             'PEP': 'PEPSI BOTTLING GROUP INC',\n",
    "             'PFE': 'PFIZER INC',\n",
    "             'PG': 'PROCTER & GAMBLE CO',\n",
    "             'R': 'RYDER SYSTEM INC',\n",
    "             'RTN': 'RAYTHEON CO/',\n",
    "             'TWX': ['AOL TIME WARNER INC', 'TIME WARNER INC'],\n",
    "             'TXN': 'TEXAS INSTRUMENTS INC',\n",
    "             'VLO': 'VALERO ENERGY CORP/TX',\n",
    "             'WFC': 'WELLS FARGO & CO/MN',\n",
    "             'WMT': 'WAL MART STORES INC',\n",
    "             'XOM': 'EXXON MOBIL CORP',\n",
    "             'XRX': 'XEROX CORP',\n",
    "             'YHOO': 'YAHOO INC'}\n",
    "\n",
    "def load_10X_summaries(cache_dir=None, force_reload=False):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "\n",
    "    filename = cache_dir / 'Loughran-McDonald_10X_Summaries_1993-2021.csv'\n",
    "\n",
    "    if (filename.is_file()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache directory: {filename}\")\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        url = 'https://docs.google.com/uc?export=download&confirm=t&id=1CUzLRwQSZ4aUTfPB9EkRtZ48gPwbCOHA'        \n",
    "        subprocess.run(f\"wget -O '{filename}' '{url}'\", shell=True, capture_output=True);\n",
    "        \n",
    "    df = pd.read_csv(filename).assign(date = lambda x: pd.to_datetime(x.FILING_DATE, format='%Y%m%d'))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "def load_ag_features(cache_dir=None, force_reload=False, sheet_name='Monthly'):\n",
    "    \"\"\"\n",
    "    load features from Amit Goyal's website:\n",
    "    https://sites.google.com/view/agoyal145\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / \"data\"\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "\n",
    "    filename = cache_dir / 'PredictorData2021.xlsx'\n",
    "\n",
    "    if (filename.is_file()) & (~force_reload):\n",
    "        print(f\"logging from cache directory: {filename}\")\n",
    "    else:\n",
    "        #logger.info(\"loading from external source\")\n",
    "        id = '1OArfD2Wv9IvGoLkJ8JyoXS0YMQLDZfY2'\n",
    "        url = f'https://docs.google.com/uc?export=download&confirm=t&id={id}'        \n",
    "        subprocess.run(f\"wget -O '{filename}' '{url}'\", shell=True, capture_output=True);\n",
    "        \n",
    "    df = pd.read_excel(filename, sheet_name=sheet_name)\n",
    "    return df.assign(date = lambda x: pd.to_datetime(x.yyyymm, format='%Y%m')).set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "lines_to_next_cell": 2
   },
   "source": [
    "### Ken French data: industry returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from ml4pmt.dataset import load_kf_returns, load_sklearn_stock_returns, load_buffets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "returns_data = load_kf_returns(\n",
    "    filename=\"12_Industry_Portfolios\", cache_dir=\"data\", force_reload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reloading from a cache directory is faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "returns_data = load_kf_returns(\n",
    "    filename=\"12_Industry_Portfolios\", cache_dir=\"data\", force_reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "returns_data_SMB_HML = load_kf_returns(\n",
    "    filename=\"F-F_Research_Data_Factors\", cache_dir=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "returns_data_MOM = load_kf_returns(filename=\"F-F_Momentum_Factor\", cache_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "returns_data_DAILY = load_kf_returns(filename=\"F-F_Research_Data_Factors_daily\", force_reload=True)['Daily']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stock returns (2003-2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "returns_data = load_sklearn_stock_returns(cache_dir=\"data\", force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from ml4pmt.metrics import sharpe_ratio\n",
    "from ml4pmt.dataset import symbol_dict\n",
    "\n",
    "start_date, end_date = returns_data.index[0].strftime('%Y-%m-%d'), returns_data.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "df = returns_data.pipe(sharpe_ratio).rename(index=symbol_dict).sort_values()\\\n",
    "            .pipe(lambda x: pd.concat([x.head(), x.tail()]))\n",
    "bar(df, horizontal=True, title=f'Annualized stock sharpe ratio: {start_date} to {end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 13F Berkshire Hathaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = load_buffets_data(cache_dir=\"data\", force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOMC Statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.dataset import load_fomc_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "statements = load_fomc_statements(force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "statements = load_fomc_statements(force_reload=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loughran-McDonalds sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.dataset import load_loughran_mcdonald_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = load_loughran_mcdonald_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOMC dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.dataset import load_fomc_change_date\n",
    "load_fomc_change_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "ml4pmt",
   "language": "python",
   "name": "ml4pmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
